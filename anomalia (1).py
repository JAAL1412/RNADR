# -*- coding: utf-8 -*-
"""anomalia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b3dwnte7w3U4zcFoLmXw9AtKbmg34Z_I

Importes
"""

import pandas as pd #manejo de datos en estructuras de tablas
import torch #libreria para red neuronal
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder #permite escalar los datos para usarse en las redes
from torch.utils.data import DataLoader, TensorDataset #permiten crear los lotes de datos
from sklearn.model_selection import train_test_split #permiten dividir en lotes para el entrenamiento
import torch.nn as nn
import torch.nn.functional as F
"""Base de datos"""

# crear data frame
data = pd.read_csv('./tr1.csv', quoting=3, on_bad_lines='skip', low_memory=False)
data['label'] = data['label'].fillna(0)
# Selecciona las columnas relevantes
columns_to_keep = [
    'biFlowEndMilliseconds',
    'biFlowStartMilliseconds',
    'destinationIPv4Address',
    'destinationTransportPort',
    'flowEndMilliseconds',
    'flowEndMilliseconds_Rev',
    'flowStartMilliseconds',
    'flowStartMilliseconds_Rev',
    'ipClassOfService',
    'octetDeltaCount',
    'octetDeltaCount_Rev',
    'packetDeltaCount',
    'packetDeltaCount_Rev',
    'protocolIdentifier',
    'sourceIPv4Address',
    'sourceTransportPort',
    'tcpControlBits',
    'tcpControlBits_Rev',
    'tcpWindowSize',
    'timestamp',

]

# Filtra las columnas seleccionadas
features = data.loc[:, columns_to_keep]
labels = data['label']
features = features.dropna()
labels = labels[features.index]

# Codificar las columnas de direcciones IP
label_encoder = LabelEncoder()
features['destinationIPv4Address'] = label_encoder.fit_transform(features['destinationIPv4Address'].astype(str))
features['sourceIPv4Address'] = label_encoder.fit_transform(features['sourceIPv4Address'].astype(str))

# Verificar y convertir cualquier columna restante que tenga valores no numéricos
for column in features.columns:
    if features[column].dtype == 'object':
        features[column] = label_encoder.fit_transform(features[column].astype(str))

# Normaliza los datos numéricos
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Dividir datos en entrenamiento y prueba
X, y = features_scaled, labels

# Dividir los datos en conjunto de entrenamiento (60%) y conjunto temporal (40%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)

# Dividir el conjunto temporal en conjunto de validación (50% de 40%) y conjunto de prueba (50% de 40%)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convertir a tensores de PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Eliminar filas con valores nan en el conjunto de entrenamiento
nan_rows = torch.isnan(X_train_tensor).any(dim=1)
X_train_tensor_clean = X_train_tensor[~nan_rows]
y_train_tensor_clean = y_train_tensor[~nan_rows]

# Verificar que se han eliminado los valores nan
print(f"Valores nan en X_train_tensor_clean: {torch.isnan(X_train_tensor_clean).sum()}")
print(f"Valores nan en y_train_tensor_clean: {torch.isnan(y_train_tensor_clean).sum()}")

# Crear DataLoader
train_dataset = TensorDataset(X_train_tensor_clean, y_train_tensor_clean)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Verificar los tamaños de los DataLoaders
print(f'Tamaño del conjunto de entrenamiento: {len(train_loader.dataset)}')
print(f'Tamaño del conjunto de validación: {len(val_loader.dataset)}')
print(f'Tamaño del conjunto de prueba: {len(test_loader.dataset)}')

"""Creación de Anoma

"""



class Net(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(Net, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # LSTM forward pass
        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)

        out, _ = self.lstm(x, (h0, c0))

        # Solo tomamos la salida del último tiempo
        out = out[:, -1, :]
        out = self.fc(out)
        return out

input_size = features.shape[1]  # Número de características de entrada
hidden_size = 70  # Tamaño de la capa oculta
num_layers = 2  # Número de capas LSTM
output_size = 2  # Asumiendo que hay 2 clases: anómalo y no anómalo

Anoma= Net(input_size, hidden_size, num_layers, output_size)
print(Anoma)

"""Se crean conjunto de entrenamiento y conjunto de prueba

Se crea el loop de entrenamiento
"""

perdida = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(Anoma.parameters(), lr=0.001)

# Bucle de entrenamiento
import time

for epoch in range(50):  # Número de épocas
    start_time = time.time()  # Tiempo de inicio de la época
    running_loss = 0.0
    correct = 0
    total = 0

    # Entrenamiento
    Anoma.train()  # Poner el modelo en modo entrenamiento
    for inputs, targets in train_loader:
        inputs = inputs.unsqueeze(1)
        optimizer.zero_grad()
        outputs = Anoma(inputs)
        loss = perdida(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calcular precisión de entrenamiento
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == targets).sum().item()
        total += targets.size(0)

    train_accuracy = 100 * correct / total
    avg_loss = running_loss / len(train_loader)

    # Validación
    Anoma.eval()  # Poner el modelo en modo evaluación
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs = inputs.unsqueeze(1)
            outputs = Anoma(inputs)
            loss = perdida(outputs, targets)
            val_loss += loss.item()

            # Calcular precisión de validación
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == targets).sum().item()
            val_total += targets.size(0)

    val_accuracy = 100 * val_correct / val_total
    avg_val_loss = val_loss / len(val_loader)

    epoch_time = time.time() - start_time  # Tiempo de la época

    print(f"Época {epoch+1}, Pérdida: {avg_loss:.4f}, Precisión: {train_accuracy:.2f}%, "
          f"Pérdida de validación: {avg_val_loss:.4f}, Precisión de validación: {val_accuracy:.2f}%, "
          f"Tiempo: {epoch_time:.2f}s")

"""Exportar modelo"""

# Guardar solo el estado del modelo
torch.save(Anoma.state_dict(), 'anomastate.pth')

# Guardar el modelo completo
torch.save(Anoma, 'anoma.pth')

